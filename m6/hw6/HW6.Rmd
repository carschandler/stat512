---
title: "HW6"
author: "Robert Chandler"
email: "chandl71@purdue.edu"
date: "2024-07-21"
output:
  pdf_document: default
---

# Setup
Begin by reading the dataset for this set of problems:

```{r setup}
le <- read.csv("../../datasets/life_expectancy.csv")
y <- le$X2015Life.expectancy
x1 <- le$Medical.doctors
x2 <- le$Nurses
x3 <- le$Pharmacists
```

# Problem 1

For the life expectancy data, Consider the MLR model given by
$Y \sim X_1 + X_2 X_3$

```{r}
model <- lm(y ~ x1 + x2 + x3)
```

## 1.a

Obtain a partial regression plot for $X_1$, $X_2$, and $X_3$ and discuss whether
the regression relationships in the fitted regression function are inappropriate
for any of the predictor variables.

```{r avplot, fig.cap="Partial Regression Plot"}
library(car)
avPlots(model)
```

The partial regression plots show that adding a linear term for $X_1$ would be a
helpful addition to the regression model already containing $X_2, X_3$. Adding
in $X_3$ to a model already containing the other variables would be a helpful
addition, but the linear increase in $Y$ as a result is not as strong as that of
$X_1$. The same goes for adding $X_2$ to a model with the others already
included. The partial plots for $X_2, X_3$ are somewhat close to zero, so
depending on what kind of confidence is desired, it is possible that these might
not be considered appropriate to add to the model, but we conclude for this case
that they are. As such, the relationships in the fitted regression function are
appropriate for all predictor variables.

## 1.b

Check for influential points by calculating DFFITS, DFBETAS and Cookâ€™s distance
values.

### DFFITS

Checking the influence on single predictions using DFFITS using the criterion:

$$
|DFFITS_i| > 2 \sqrt{\frac{p}{n}}
$$

```{r}
n <- length(y)
p <- length(model$coefficients)
influential_dffits <- abs(dffits(model)) > 2 * sqrt(p / n)
```

There are `r sum(influential_dffits)` influential points according to the DFFITS
criterion. The following rows in the dataset (starting at an index of 1) are
influential: $i = \{`r which(influential_dffits)`\}$.

### DFBETAS

Checking the influence on the regression coefficients using DFBETAS using the
criterion:

$$
|DFBETAS_k(i)| > \frac{2}{\sqrt{n}}
$$

```{r}
influential_dfbetas <- abs(dfbetas(model)) > 2 / sqrt(n)
```

There are `r sum(influential_dfbetas)` total influential points according to the
DFBETAS criterion. The breakdown of how many correspond to each coefficient is:

```{r}
colSums(influential_dfbetas)
```

The following shows which cases are influential on which regression parameters:

```{r}
influential_dfbetas[rowSums(influential_dfbetas) != 0, ]
```

The row/index of the case is shown on the left, and the four columns correspond
to whether or not that case is influential on the parameter labeled in the
respective column header.

### Cook's Distance

Checking which cases are influential on all fitted values using Cook's distance
with the criterion:

```{r}
f_crit <- qf(0.2, p, n - p)
```

$$
D_i > F(0.2, p, n - p) = F(0.2, `r p`, `r n - p`) = `r f_crit`
$$

```{r}
influential_cooks <- cooks.distance(model) > f_crit
```

There are `r sum(influential_cooks)` influential cases under the criterion for
Cook's distance, so we conclude that there are no cases in the dataset that are
significantly influential on *all* fitted values.

## 1.c

Determine whether there is multicollinearity present based on VIF.

```{r}
car::vif(model)
```

All of the VIF terms are between 2 and 3 with a mean of $`r mean(vif(model))`$
which is much less than 10, which is the point at which we would consider there
to be a serious multicollinearity issue. These values are still somewhat greater
than 1, and the mean value indicates that the expected sum of the squared errors
in the LS standardized regression coefficients is $`r mean(vif(model))`$ times
what it would be if the $X$ variables were uncorrelated. Therefore, we conclude
that while there may be some multicollinearity present, it definitely should not
be classified as a serious issue.

# Problem 2

Use R to summarize the OLS model. Then compute the confidence interval for the
linear impact of $X_1$, $X_2$, and $X_3$.

Showing the R summary of the previously calculated OLS model:

```{r}
summary(model)
```

Computing the (95%) confidence interval for the linear impact of each input variable:

```{r}
confint(model)[2:4, ]
```

# Problem 3

Use R to complete the WLS analysis on the model (note: just do one iteration).
Discuss the difference. Then compute the confidence interval for the
linear impact of $X_1$, $X_2$, $X_3$.

```{r}
w <- 1 / fitted(lm(abs(residuals(model)) ~ x1 + x2 + x3))^2
model_wls <- lm(y ~ x1 + x2 + x3, weight = w)
summary(model_wls)
```

- The estimated values of the parameters are very similar in WLS to the OLS
model.
- The standard errors have decreased slightly for the parameters, the t-values
are comparable to OLS, and the p-values have all decreased, meaning the
estimates on our parameters have become more significant.
- The residual standard error should not be compared as the residuals have been
modified. 
- The R-squared values have remained nearly exactly the same with the values for
WLS dropping just slightly.
- The global F-statistic is nearly the same as well, dropping slightly in the
WLS model.

The confidence interval for the linear impact of the input variables is:

```{r}
confint(model_wls)[2:4, ]
```

This interval is very similar to the previous one, but has tightened up just a
bit more than the OLS interval.

# Problem 4

Use R to complete the Robust analysis on the model. Then compute the confidence
interval for the linear impact of $X_1$, $X_2$, and $X_3$.

```{r}
library(MASS)
model_rlm <- rlm(y ~ x1 + x2 + x3, psi = psi.bisquare)
summary_rlm <- summary(model_rlm)
summary_rlm
```

- The Robust model is once again very similar to the previous models.
- It shares similar parameter estimates to the other two models.
- This time, the standard errors of the parameters *increased* slightly relative
to the values in the OLS model.
- The residuals have once more been modified, so the residual standard error
cannot be compared to OLS, but the value is similar to that of the WLS this
time.
- The t-values are once again very similar to the previous values.


```{r}
ci_rlm <- function(model, alpha = 0.05) {
  stopifnot(any(class(model) == "rlm"))

  n <- nrow(model$model)
  p <- ncol(model$model)
  t_crit <- qt(1 - alpha / 2, n - p)
  summary_model <- summary(model)
  b <- drop(summary_model$coefficients[, "Value"])
  stderr <- drop(summary(model)$coefficients[, "Std. Error"])
  interval_lower <- b - t_crit * stderr
  interval_upper <- b + t_crit * stderr
  intervals <- t(rbind(interval_lower, interval_upper))
  colnames(intervals) <- c(
    paste(100 * alpha / 2, "%"), paste(100 * (1 - alpha / 2), "%")
  )
  intervals
}

intervals_rlm <- ci_rlm(model_rlm)
```

The confidence intervals for the linear impact of the input variables are shown
below:

$$
\begin{gathered}
\beta_1: (`r intervals_rlm["x1", ]`) \\
\beta_2: (`r intervals_rlm["x2", ]`) \\
\beta_3: (`r intervals_rlm["x3", ]`) \\
\end{gathered}
$$


# Problem 5

Apply the bootstrapping method to compute the confidence interval on the
parameters for WLS, OLS, and Robust.

## OLS

```{r}
boot_ols <- function(data, indices) {
  data <- data[indices, ]
  model_ols <- lm(y ~ x1 + x2 + x3, data)
  coef(model_ols)
}

boot_ols <- boot(data = df_le, statistic = boot_ols, R = 100)
boot_ols

ci_x0 <- boot.ci(boot_ols, index = 1, type = "perc")$percent[4:5]
ci_x1 <- boot.ci(boot_ols, index = 2, type = "perc")$percent[4:5]
ci_x2 <- boot.ci(boot_ols, index = 3, type = "perc")$percent[4:5]
ci_x3 <- boot.ci(boot_ols, index = 4, type = "perc")$percent[4:5]
```

The confidence intervals on the parameters are:

$$
\begin{gathered}
\beta_0: (`r ci_x0`) \\
\beta_1: (`r ci_x1`) \\
\beta_2: (`r ci_x2`) \\
\beta_3: (`r ci_x3`) \\
\end{gathered}
$$

## WLS

```{r}
df_le <- data.frame(y, x1, x2, x3)
library(boot)

boot_wls <- function(data, indices) {
  data <- data[indices, ]
  model <- lm(y ~ x1 + x2 + x3, data)
  w <- 1 / fitted(lm(abs(residuals(model)) ~ x1 + x2 + x3, data))^2
  model_wls <- lm(y ~ x1 + x2 + x3, weight = w, data = data)
  coef(model_wls)
}

boot_wls <- boot(data = df_le, statistic = boot_wls, R = 100)
boot_wls

ci_x0 <- boot.ci(boot_wls, index = 1, type = "perc")$percent[4:5]
ci_x1 <- boot.ci(boot_wls, index = 2, type = "perc")$percent[4:5]
ci_x2 <- boot.ci(boot_wls, index = 3, type = "perc")$percent[4:5]
ci_x3 <- boot.ci(boot_wls, index = 4, type = "perc")$percent[4:5]
```

The confidence intervals on the parameters are:

$$
\begin{gathered}
\beta_0: (`r ci_x0`) \\
\beta_1: (`r ci_x1`) \\
\beta_2: (`r ci_x2`) \\
\beta_3: (`r ci_x3`) \\
\end{gathered}
$$

## Robust

```{r}
library(boot)
boot_huber <- function(data, indices, maxit = 100) {
  data <- data[indices, ]
  model_rlm <- rlm(y ~ x1 + x2 + x3, data = data, maxit = maxit)
  coef(model_rlm)
}

boot_rlm <- boot(
  data = df_le,
  statistic = boot_huber,
  R = 100,
  maxit = 100,
)
boot_rlm

ci_x0 <- boot.ci(boot_rlm, index = 1, type = "perc")$percent[4:5]
ci_x1 <- boot.ci(boot_rlm, index = 2, type = "perc")$percent[4:5]
ci_x2 <- boot.ci(boot_rlm, index = 3, type = "perc")$percent[4:5]
ci_x3 <- boot.ci(boot_rlm, index = 4, type = "perc")$percent[4:5]
```

The confidence intervals on the parameters are:

$$
\begin{gathered}
\beta_0: (`r ci_x0`) \\
\beta_1: (`r ci_x1`) \\
\beta_2: (`r ci_x2`) \\
\beta_3: (`r ci_x3`) \\
\end{gathered}
$$

# Problem 6

Note that the lm.ridge() algorithm turns out to be indeterminate when VIF is
close to 1, resulting in a flat ridge trace line. On the other hand, the
`lmridge()` function only reports $\lambda = 0$ (i.e., OLS). For practicing the
`lm.ridge()` and `lmridge()` functions, this question will be to use the body
fat example and complete the confidence interval for $X_1$, $X_2$, and $X_3$ in
the ridge model.

We can first read in the dataset and model it using `MASS::lm.ridge()`, then show the
ridge trace plot.

```{r fig.cap = "Ridge Trace Plot from MASS Package"}
fat <- read.csv("../../datasets/bodyfat.csv")
colnames(fat) <- c("x1", "x2", "x3", "y")
model_ridge_mass <- lm.ridge(
  y ~ x1 + x2 + x3,
  data = fat, lambda = seq(0, 1, 0.01)
)
plot(model_ridge_mass)
```

Next, we can select the $\lambda$ value that has the smallest GCV value.

```{r}
select(model_ridge_mass)
lambda_opt <- 0.02
```

The optimal $\lambda$ value which minimizes the Generalized Cross Validation
equation is $\lambda = `r lambda_opt`$.

Next, we can model the data using `lmridge::lmridge()` and show the ridge trace
plot.

```{r fig.cap = "Ridge Trace Plot from lmridge Package"}
model_lmridge <- lmridge::lmridge(
  y ~ x1 + x2 + x3,
  data = fat, K = seq(0, 1, 0.01)
)
plot(model_lmridge)
```

We observe the same plot as before, just with different axis limits. Next, we
observe the VIF values for the first six values of $\lambda$.

```{r}
head(lmridge::vif(model_lmridge))
```

The VIF are closest to 1 for all parameters when $\lambda = k = 0.02$.

Lastly, we observe the model summary for the optimal $\lambda$ value:

```{r}
ridge_summary <- summary(lmridge::lmridge(
  y ~ x1 + x2 + x3,
  data = fat, K = lambda_opt
))
ridge_summary
```

We can use the values in this summary to calculate the confidence interval for
the parameters:

```{r}
estimates <- ridge_summary$summaries$`summary  1`$coefficients[
  c("x1", "x2", "x3"), "Estimate (Sc)"
]

stderrs <- ridge_summary$summaries$`summary  1`$coefficients[
  c("x1", "x2", "x3"), "StdErr (Sc)"
]

df_ridge <- ridge_summary$summaries$`summary  1`$df1[, "df ridge"]
alpha <- 0.05
n <- nrow(fat)

interval_lower <- estimates - qt(1 - alpha / 2, n - df_ridge) * stderrs
interval_upper <- estimates + qt(1 - alpha / 2, n - df_ridge) * stderrs
```

The intervals are as follows:

For $X_1$:

$$
\begin{gathered}
b_{1,SC} \pm t(1 - \alpha / 2, n - df_{\text{ridge}}) s_{1,SC} \\
 `r estimates["x1"]` \pm t(`r 1 - alpha / 2`, `r n` - `r df_ridge`)
`r stderrs["x1"]` \\
 (`r interval_lower["x1"]`, `r interval_upper["x1"]`)
\end{gathered}
$$

Where $b_{SC}, s_{SC}$ refer to the scaled estimate for the parameter $\beta_1$
and the scaled standard error of the parameter.

For $X_2$:

$$
\begin{gathered}
b_{2,SC} \pm t(1 - \alpha / 2, n - df_{\text{ridge}}) s_{2,SC} \\
 `r estimates["x2"]` \pm t(`r 1 - alpha / 2`, `r n` - `r df_ridge`)
`r stderrs["x2"]` \\
 (`r interval_lower["x2"]`, `r interval_upper["x2"]`)
\end{gathered}
$$

For $X_3$:

$$
\begin{gathered}
b_{3,SC} \pm t(1 - \alpha / 3, n - df_{\text{ridge}}) s_{3,SC} \\
 `r estimates["x3"]` \pm t(`r 1 - alpha / 2`, `r n` - `r df_ridge`)
`r stderrs["x3"]` \\
 (`r interval_lower["x3"]`, `r interval_upper["x3"]`)
\end{gathered}
$$
