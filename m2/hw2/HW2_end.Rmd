---
title: "HW2"
author: "Robert Chandler"
email: "chandl71@purdue.edu"
date: "2024-06-23"
output:
  pdf_document: default
  html_document: default
---

# Setup
Begin by reading the dataset for this set of problems:
```{r setup}
le <- read.csv("./life_expectancy.csv")
```

```{r}
x <- le$Nurses
y <- le$X2015Life.expectancy
```

# Problem 2
For a two-sided hypothesis test on the significance of the linear correlation coefficient between $X$ and $Y$, $H_0: \rho = 0, H_a: \rho \ne 0$...

## 2.a
If a T-test is used, the test statistic is computed with the sample correlation, $r$, with the formula


$$
t^{*} = \frac{r \sqrt{ n-2 }}{\sqrt{ 1 - r^{2} }}
$$

which is computed as

```{r}
r <- cor(x, y)
n <- length(x)
t_star <- r * sqrt(n - 2) / sqrt(1 - r^2)
```

$$
t^{*} = \frac{`r r` \sqrt{ `r n`-2 }}{\sqrt{ 1 - `r r`^{2} }}
= `r t_star`
$$

The critical t-value is

```{r}
t_crit <- qt(1 - alpha / 2, n - 2)
```

$$
t(1 - \alpha / 2, n - 2) = t(`r 1 - alpha / 2`, `r n - 2`) = `r t_crit` <
`r t_star` = t^*
$$

so we can reject $H_0$ and conclude that $X, Y$ are linearly associated.

## 2.b
Is this test statistic the same as the t-test in part (a)? **YES**.

## 2.c
Discuss when the results of the hypothesis test on the linear impact and the linear
association are equivalent. 

Tests on linear impact and linear association are equivalent when the
population is bivariate normal, but the regression model still holds even if
they are not bivariate normal, so long as the conditional distributions
$Y_i|X_i$ are normal and independent, with conditional means $\beta_0 + \beta_1
X_i$ and conditional variance $\sigma^2$ and the $X_i$ are independent random
variables whose distribution does not depend on $\beta_0, \beta_1, \sigma^2$. In
this case, the results of the hypothesis tests are still equivalent.

## 2.d
Use R to compute a 95% confidence interval for the linear correlation
coefficient between $Y$ and $X$. Use the confidence interval to verify the
hypothesis test in (c). (hint: if the confidence interval contains the
hypothesized value, then the two-sided hypotheses should be rejected or not?) 

We need to transform our data using a Fisher z transformation:

$$
\begin{aligned}
z' &= \frac{1}{2} log_e(\frac{1 + r_{12}}{1 - r_{12}}) \\
\sigma ^2 \{z'\} &= \frac{1}{n-3}
\end{aligned}
$$


```{r}
alpha <- 0.05
z_prime <- 0.5 * log((1 + r) / (1 - r))
sigma_z_prime <- 1 / sqrt(n - 3)
z <- qnorm(1 - alpha / 2)
z_prime_upper <- z_prime + z * sigma_z_prime
z_prime_lower <- z_prime - z * sigma_z_prime
r_upper <- (exp(2 * z_prime_upper) - 1) / (exp(2 * z_prime_upper) + 1)
r_lower <- (exp(2 * z_prime_lower) - 1) / (exp(2 * z_prime_lower) + 1)
```

The transformed confidence interval for $\rho$ is then

$$
z' \pm z(1 - \alpha/2) \sigma\{z'\} = 
`r z_prime` \pm z(1 - `r alpha`/2) `r sigma_z_prime` =
(`r z_prime_upper`, `r z_prime_lower`)
$$

and transforming back using $r = (e^{2z'} - 1) / (e^{2z'} + 1)$, the final
interval is

$$
(`r r_upper`, `r r_lower`)
$$

and since $r = 0$ does not fall within this interval, we reject $H_0$ and
conclude that the data are linearly associated.

# Problem 3
Consider a simple linear regression model with $Y \sim X$ on the following table. Practice doing the lack-of-fit by hand.

```{r}
library(knitr)
data_p3 <- data.frame(
  x = c(0.24, 0.22, 0.23, 0.24, 0.24, 0.24, 0.22, 0.21, 0.24, 0.21),
  y = c(16, 40, 32, 13, 1, 1, 2, 3, 8, 14)
)
x <- data_p3$x
y <- data_p3$y
kable(df, caption = "Problem 3 Dataset")
```

## 3.a

Based on a (R-generated) scatter plot of $X$ and $Y$ with the regression line,
comment on whether the Simple Linear Regression (SLR) exhibits a lack-of-fit
issue. 

Plot the SLR curve of $Y$ regressed on $X$:

```{r fig.cap =  "Simple Linear Regression of $Y \\sim X$"}
model <- lm(y ~ x)
plot(y ~ x)
abline(model)
```
<!-- ggplot(data_p3, aes(x = x, y = y)) + -->
<!--   geom_point() + -->
<!--   stat_smooth(method = lm, se = FALSE, ) + -->
<!--   labs(caption = "Simple Linear Regression of $Y \\sim X$") -->

This is a difficult case to judge at a first glance. First of all, we note that
$X$ does not seem to have much of an impact on $Y$. Regarding the goodness of
fit, the points do seem to have a decent amount of scatter about the line, but
this could just imply a high variance in the error. It does not necessarily seem
that a linear model is incorrect, but it is still tough to judge by eye, so
this is a great use case for the lack-of-fit test.

## 3.b
Compute the components for the lack of fit test: $c$, $\hat{Y}_i$, $\bar{Y}_i$,
$\bar{Y}$, $SSPE$, $SSLF$, $SSE$, $DFPE$, $DFLF$, $DFE$.

Extract coefficients from the model first:


```{r}
b0 <- model$coefficients[["(Intercept)"]]
b1 <- model$coefficients[["x"]]
```

Compute requested values:

```{r}
n <- length(x)

df_unq <- aggregate(y ~ x, data_p3, mean)
colnames(df_unq)[2] <- "ybar"

c <- length(df_unq$x)
df_unq$yhat <- sapply(df_unq$x, function(x) b0 + b1 * x)

ybar_overall <- mean(y)

# Build up SS values iteratively
sspe <- 0
sslf <- 0
sse <- 0
for (i in seq_along(y)) {
  # Find the ybar and yhat values associated with the current y... this takes
  # care of the summation across the i index since these are already
  # pre-calculated
  ybar_i <- df_unq$ybar[df_unq$x == x[i]]
  yhat_i <- df_unq$yhat[df_unq$x == x[i]]

  # This is good, but FIXME on yhat
  spe <- (y[i] - ybar_i)^2
  sspe <- sspe + spe

  slf <- (ybar_i - yhat_i)^2
  sslf <- sslf + slf

  se <- (y[i] - yhat_i)^2
  sse <- sse + se
}

# Ensure SSPE + SSLF = SSE
stopifnot(sspe + sslf - sse < 1e-10)

dfpe <- n - c
dflf <- c - 2
dfe <- n - 2
```

The resulting values are:

$$
\begin{aligned}
c &= `r c` \\
\hat{Y}_i &= `r df_unq$yhat` \\
\bar{Y}_i &= `r df_unq$ybar` \\
\bar{Y} &= `r ybar_overall` \\
SSPE &= \sum \sum (Y_{ij} - \bar{Y}_i)^2 = `r sspe` \\
SSLF &= \sum \sum (\bar{Y}_i - \hat{Y}_{ij})^2 = `r sslf` \\
SSE &= \sum \sum (Y_{ij} - \hat{Y}_ij)^2 = `r sse` \\
DFPE &= `r dfpe` \\
DFLF &= `r dflf` \\
DFE &= `r dfe` \\
\end{aligned}
$$

## 3.c

Next, consider the lack-of-fit test for the SLR. Define $H_0, H_a$, calculate
test statistic, define reject region, compute the p-value, and state the
conclusion. 

The hypotheses are as follows:

$$
\begin{gathered}
H_0: E\{ Y \} = \beta_0 + \beta_1 X \\
H_a: E\{ Y \} \ne \beta_0 + \beta_1 X \\
\end{gathered}
$$

The test statistic is

```{r}
mslf <- sslf / (c - 2)
mspe <- sspe / (n - c)
f_star <- mslf / mspe
```

$$
F^* = \frac{SSE - SSPE}{(n - 2) - (n - c)} \div \frac{SSPE}{n - c} = \frac{SSLF}{c - 2} \div \frac{SSPE}{n - c} = \frac{`r sslf`}{`r c` - 2} \div \frac{`r sspe`}{`r n` - `r c`} = `r f_star`
$$

and the critical $F$ value is

```{r}
f_crit <- qf(1 - alpha, c - 2, n - c)
```

$$
F(1 - \alpha; c-2, n-c) = `r f_crit`
$$

The rejection region is:

$$
\begin{aligned}
\text{If } F^* &\le F(1 - \alpha; c-2, n-c)\text{, fail to reject } H_0 \\
\text{If } F^* &> F(1 - \alpha; c-2, n-c)\text{, reject } H_0
\end{aligned}
$$

The p-value is

```{r}
p_f_star <- 1 - pf(f_star, c - 2, n - c)
```

$$
p = P\{F(c-2, n - c) > F^*\} = P\{F(`r c-2`, `r n - c`) > `r f_star`\} =
`r p_f_star`
$$

so, because $F^*$ and $p$ both fall outside of the rejection region, we fail to
reject $H_0$, and we conclude that **the model does not have a lack-of-fit
problem** at $\alpha = 0.05$.

## 3.d

Utilize R to conduct the lack-of-fit test and identify as many components as
possible from the ones computed in part (b) in the R output.

Our hand-calculated values above are validated in the ANOVA table below:

```{r}
anova(model, lm(y ~ as.factor(x), data_p3))
```

The values for $SSE$ and $SSPE$ are readily found in the first and second rows
of the `RSS` column, respectively, and $SSLF$ is in the `Sum of Sq` column. The
different DOF values are all in the table: $DFE$ and $DFPE$ are in the first and
second rows of the `Res.Df` column, respectively, and $DFLF$ is in the `Df`
column. $c$ can be found indirectly from $c = 2 + DFLF$. The rest of the values
in [3.b] are derived from $Y$ and are not shown in the table.

# TODO: make sure to double check hand-typed numbers from tables in p3,4

# Problem 4

Consider a lack of fit test on the following data on Y ~ X:

```{r}
data_p4 <- data.frame(
  x = c(0.19, 0.44, 0.35, 0.32, 0.29),
  y = c(65, 30, 22, 31, 9)
)
kable(data_p4)
```

# 4.a

Can you perform a lack of fit test on this data?

As the data currently stands, a lack of fit test cannot be performed on it
because there are no replicate values in $X$. However, we can create a kind of
pseudo-replicate dataset by grouping the data if we needed to perform the test
and could not gather any new samples.

# 4.b

Suppose a new row is added: X 0.19, Y 59 and the sample size is now 6. Before,
the $SSPE$ was simply $0$ because we had no replicates, and now it is just a
function of the $X=0.19$ terms:

```{r}
data_p4 <- rbind(data_p4, c(0.19, 59))
y_i <- data_p4[data_p4$x == 0.19, "y"]
sspe <- sum(sapply(y_i, function(y) (y - mean(y_i))^2))
```

$$
SSPE = (Y_{ij} - \bar{Y}_i)^2 = `r sspe`
$$

so $SSPE$ has **increased** by $`r sspe`$.

```{r}
n <- nrow(data_p4)
c <- length(unique(data_p4$x))
dfpe <- n - c
dflf <- c - 2
```

$DFPE = n - c = `r dfpe`$ and $DFLF = c - 2 = `r dflf`$.

## 4.c
Suppose the data is grouped by the tenth digits as follows,
X Y
0.1 65
0.4 30
0.3 22
0.3 31
0.2 9
Then the SSPE is ________(increased/decreased) by ______; the dfPE=_____________ and
dfLF=_____________.
